### 聚类
聚类算法是非监督学习算法，将并没有明确的标签数据中，输入算法中，找到这些数据中

的内在联系，一个能够找到圈出这些点集，被称为聚类算法

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/聚类1.PNG?raw=true)


### K-MEANS
    - 算法接受参数K，然后将事先输入的n个数据
    对象划分为k个聚类以便使得所获得的聚类满足：
    同一聚类中的对象相似度较高，
    而不同聚类中的对象相似度较小
    
    - 算法思想：以空间中k哥点为中心进行聚类，
    对最靠近他们的对象归类。
    通过迭代的方法，逐次更新各聚类中心得值，
    直至得到最好的聚类结果
    
具体步骤：

    1. 先从没有标签的元素集合A中随机取K个元素，作为K个子集各自的重心
    2. 分别计算剩下的元素到k个子集重心的距离（这里的距离也可以使用欧式距离）,
    根据距离将这些元素分别划归到最近的子集
    3. 根据聚类结果，重新计算重心（重心的计算方法是计算子集中所以元素各个维度的算术平均值）
    4.将集合A中全部元素按照新的重心然后再重新聚类
    5. 重复第4步， 直到聚类结果不在发生变化
    

如下有四个点，假设取（1,1）（2,1）为两个分类的中心点
![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/聚类2.PNG?raw=true)

然后计算所有的点到中心的距离：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/聚类3.PNG?raw=true)

> 第一行是所有点到第一类点的距离，第二行是所有点到第二类点的距离

然后根据距离的大小将四个点进行分类

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/聚类4.PNG?raw=true)

> 第一行代表第一个类别，第二行代表第二个类别


然后重新计算重心：

第二个类别C2：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/聚类5.PNG?raw=true)

然后重复以上步骤：
![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/聚类6.PNG?raw=true)

直到聚类不发生变化

### Mini Batch K-Means
Mini Batch K-means算法是k-Means算法的变种，采用小批量的数据子集减少计算时间，这里所谓的小批量是指每次训练算法时所随机抽取的数据子集，

采用这些随机产生的子集进行训练算法，大大减少了计算时间，结果一般只略差于标准算法。

该算法的迭代步骤有两步：

1. 从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心
2. 更新质心， 与k均值算法相比，数据的更新是在每一个的样本集上。mini Batch K-Means 比K-means有更快的，收敛速度，
但同时也降低了聚类的效果，但是在实际项目中却表现的不明显。


### K-MEANS算法分析
k-means算法在有些情况下是会出现问题的：

1. 对k个初始质心得选择比较敏感，容易陷入局部最小值。例如，我们上面的算法运行的时候，有可能会得到不同的结果，如下面这两种情况。K-means也是收敛了，只是收敛了局部最小值：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/kmeans1.PNG?raw=true)

解决办法：

    使用多次的随机初始化，计算每一次建模得到的代价函数的值，选取代价函数最小结果作为聚类结果

    for i = 1 to 100 {
        Randomly initialize K-means
        Run k-means.et
        Compute cost function(distortion)  

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/kmeans4.PNG?raw=true)

2. k的值选择是用户决定的，不同的k得到的结果会有挺大的不同，如图所示，左边是k = 3 的结果，蓝色的簇太稀疏了，蓝色的簇应该可以在划分成两个簇。右边是k=5的结果，红色和蓝色的簇应该合并成为一个簇

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/kmeans2.PNG?raw=true)


> 解决办法：

    肘部法则：使用肘部法则来选取k值

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/肘部法则1.PNG?raw=true)
    

3. 存在局限性，如下这种非球状的数据结构分布不适用：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/kmeans3.PNG?raw=true)

4. 数据量比较大的时候，收敛会比较慢

### 可视化K-MEANS

https://www.naftaliharris.com/blog/visualizing-k-means-clustering

### 基于密度的方法：DBSCAN
DBSCAN = Density-Based Spatial Clustering of Applications with Noise

该算法将具有足够高密度的区域划分为簇，并可以发现任何形状的聚类

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/DBSCAN1.PNG?raw=true)



![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week-8/image/DBSCAN2.PNG?raw=true)

ε领域：

    给定对象半径ε内的区域称为该对象的ε领域



核心对象：
    
    如果给定ε领域内的样本点数大于等于Minpoints,则该对象为核心对象

直接密度可达：


    直接密度可达：给定一个对象集合D，如果p在q的ε领域内，且q是一个核心对象，
    则我们说对象p从q出发是直接密度可达的（directly density-reachable）
密度可达：
    
    集合D，存在一个对象链
    p1，p2....pn，p1=q，pn=p,pi+1z是从pi关于ε和Minpoints直接密度可达，
    则称点p是q关于ε和Minpoints密度可达的。
密度相连：

    集合D存在点o，使得点p，q是从o关于ε和MinPoints密度可达的，那么点p，q是关于ε和Minpoints
    密度相连的。


### DBSCAN算法思想
1. 指定合适的ε和Minpoints
2. 计算所有的样本点，如果点p的ε邻域里有超过Minpoints个点，则创建一个以p为核心点的新簇
3. 反复寻找那些核心点直接密度可达（之后可能是密度可达）的点，将其加入到相应的簇，对于核心点发生“密度相连”状况的簇，予以合并
4. 当没有新的点可以被添加到任何簇时，算法结束

算法缺点：
    
    - 当数据量增大时，要求较大的内存支持，i/o消耗也很大
    - 当空间聚类的密度不均匀，聚类间距相差很大时，聚类质量较差

DBSCAN和K-MEANS比较：
    
    - DBSCAN不需要输入聚类个数
    - 聚类簇的形状没有要求
    - 可以在需要时输入过滤噪声的参数
    

